{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hiteshove/B.-Pre-processing-Quality-Filters-/blob/main/Copy_of_Welcome_to_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YnjFZEeaKR-0"
      },
      "outputs": [],
      "source": [
        "# Multimodal RAG Chatbot (Colab)\n",
        "**What this notebook does:** ingest images/PDFs/audio/video ‚Üí OCR/ASR/transcription ‚Üí index with FAISS ‚Üí answer queries using Gemini 2.5-flash (RAG) with source citations and related documents.\n",
        "\n",
        "**Before you start:** have your Gemini 2.5-flash API key ready.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "YSUjIq89KTmE",
        "outputId": "eb02e41b-ac61-4faa-fbb8-37b68b3fc68b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting it-core-news-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_lg-3.8.0/it_core_news_lg-3.8.0-py3-none-any.whl (567.9 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m567.9/567.9 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('it_core_news_lg')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.12/dist-packages (20250625)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (3.4.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.12/dist-packages (from triton>=2->openai-whisper) (75.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.11.1.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "‚úÖ Installs finished. If tesseract Italian data isn't present, OCR quality for Italian may be reduced.\n"
          ]
        }
      ],
      "source": [
        "# Run this cell once at the top of the notebook.\n",
        "# Installs python packages and system packages needed for OCR/FFmpeg.\n",
        "!pip install -q google-generativeai pytesseract Pillow PyMuPDF faiss-cpu sentence-transformers transformers spacy networkx rdflib\n",
        "# Italian spaCy model\n",
        "!python -m spacy download it_core_news_lg\n",
        "!pip install openai-whisper\n",
        "\n",
        "# System packages (FFmpeg, Tesseract + Italian language pack if available)\n",
        "# NOTE: Debian/Ubuntu names may vary; Colab should have ffmpeg and tesseract already but we ensure it.\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq ffmpeg tesseract-ocr tesseract-ocr-ita || true\n",
        "\n",
        "print(\"‚úÖ Installs finished. If tesseract Italian data isn't present, OCR quality for Italian may be reduced.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3L0V9yUKplY",
        "outputId": "dc44cca2-9dec-4089-b098-d2629a414c53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paste your Gemini 2.5-flash API key (hidden): ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "‚úÖ Gemini configured.\n"
          ]
        }
      ],
      "source": [
        "# Securely input your Gemini API key (it won't be printed)\n",
        "from getpass import getpass\n",
        "import google.generativeai as genai\n",
        "\n",
        "GEMINI_KEY = getpass(\"Paste your Gemini 2.5-flash API key (hidden): \").strip()\n",
        "if not GEMINI_KEY:\n",
        "    raise SystemExit(\"Gemini API key is required to proceed.\")\n",
        "\n",
        "genai.configure(api_key=GEMINI_KEY)\n",
        "gemini_model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "print(\"‚úÖ Gemini configured.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTMRdQRfLAg1",
        "outputId": "caa00dd1-33fb-4f51-eb66-7328139f2071"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Environment ready.\n"
          ]
        }
      ],
      "source": [
        "# Core imports and workspace setup\n",
        "import os, io, uuid, sqlite3, subprocess\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import fitz  # PyMuPDF\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import spacy\n",
        "import networkx as nx\n",
        "import google.generativeai as genai  # already configured above\n",
        "\n",
        "# Directories\n",
        "os.makedirs(\"uploads\", exist_ok=True)\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "\n",
        "# DB setup\n",
        "conn = sqlite3.connect(\"metadata.db\")\n",
        "c = conn.cursor()\n",
        "c.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS assets (\n",
        "    id TEXT PRIMARY KEY,\n",
        "    filename TEXT,\n",
        "    type TEXT,\n",
        "    text_content TEXT,\n",
        "    created_at TEXT\n",
        ")\n",
        "\"\"\")\n",
        "conn.commit()\n",
        "\n",
        "# Embedding model\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "EMB_DIM = embed_model.get_sentence_embedding_dimension()\n",
        "\n",
        "# FAISS files (persistence)\n",
        "FAISS_INDEX_FILE = \"outputs/faiss.index\"\n",
        "IDMAP_FILE = \"outputs/id_map.npy\"   # store python list as numpy object? we'll use pickle\n",
        "import pickle\n",
        "IDMAP_PICKLE = \"outputs/id_map.pkl\"\n",
        "\n",
        "# NER model\n",
        "nlp = spacy.load(\"it_core_news_lg\")\n",
        "\n",
        "# Graph\n",
        "G = nx.Graph()\n",
        "\n",
        "print(\"‚úÖ Environment ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNDL6tyTLg4R"
      },
      "outputs": [],
      "source": [
        "import pytesseract, fitz, uuid, subprocess, os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "import sqlite3\n",
        "import google.generativeai as genai\n",
        "\n",
        "# Setup SQLite DB\n",
        "conn = sqlite3.connect(\"metadata.db\")\n",
        "c = conn.cursor()\n",
        "c.execute(\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS assets (\n",
        "    id TEXT PRIMARY KEY,\n",
        "    filename TEXT,\n",
        "    type TEXT,\n",
        "    text_content TEXT,\n",
        "    created_at TEXT\n",
        ")\n",
        "\"\"\")\n",
        "conn.commit()\n",
        "\n",
        "# Gemini models\n",
        "flash_model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "pro_model = genai.GenerativeModel(\"gemini-1.5-pro\")  # needed for audio transcription\n",
        "\n",
        "def save_asset(asset):\n",
        "    c.execute(\"\"\"\n",
        "    INSERT OR REPLACE INTO assets (id, filename, type, text_content, created_at)\n",
        "    VALUES (?, ?, ?, ?, ?)\n",
        "    \"\"\", (asset[\"id\"], asset[\"filename\"], asset[\"type\"], asset[\"text\"], asset[\"created_at\"]))\n",
        "    conn.commit()\n",
        "\n",
        "# IMAGE ingestion\n",
        "def ingest_image(path):\n",
        "    text = pytesseract.image_to_string(Image.open(path), lang=\"ita+eng\")\n",
        "    asset = {\"id\": str(uuid.uuid4()), \"filename\": Path(path).name, \"type\": \"image\",\n",
        "             \"text\": text, \"created_at\": datetime.now().isoformat()}\n",
        "    save_asset(asset)\n",
        "    return asset\n",
        "\n",
        "# PDF ingestion\n",
        "def ingest_pdf(path):\n",
        "    doc = fitz.open(path)\n",
        "    text = \"\\n\".join([p.get_text() for p in doc])\n",
        "    asset = {\"id\": str(uuid.uuid4()), \"filename\": Path(path).name, \"type\": \"document\",\n",
        "             \"text\": text, \"created_at\": datetime.now().isoformat()}\n",
        "    save_asset(asset)\n",
        "    return asset\n",
        "\n",
        "# AUDIO ingestion (NEW with Whisper + Gemini fallback)\n",
        "def ingest_audio(path):\n",
        "    text = None\n",
        "    try:\n",
        "        import whisper\n",
        "        model = whisper.load_model(\"small\")\n",
        "        result = model.transcribe(path, language=\"it\")\n",
        "        text = result[\"text\"]\n",
        "        print(f\"‚úÖ Transcribed with Whisper: {Path(path).name}\")\n",
        "    except ImportError:\n",
        "        print(f\"‚ö† Whisper not installed ‚Üí using Gemini Pro for {Path(path).name}\")\n",
        "        with open(path, \"rb\") as f:\n",
        "            audio_data = f.read()\n",
        "        resp = pro_model.generate_content([\n",
        "            {\"mime_type\": \"audio/wav\", \"data\": audio_data},\n",
        "            \"Trascrivi questo audio in italiano\"\n",
        "        ])\n",
        "        text = resp.text\n",
        "\n",
        "    asset = {\"id\": str(uuid.uuid4()), \"filename\": Path(path).name, \"type\": \"audio\",\n",
        "             \"text\": text, \"created_at\": datetime.now().isoformat()}\n",
        "    save_asset(asset)\n",
        "    return asset\n",
        "\n",
        "# VIDEO ingestion (extract audio first, then transcribe)\n",
        "def extract_audio_from_video(video_path, audio_out=\"temp_audio.wav\"):\n",
        "    cmd = f'ffmpeg -y -i \"{video_path}\" -vn -acodec pcm_s16le -ar 16000 -ac 1 \"{audio_out}\"'\n",
        "    subprocess.call(cmd, shell=True)\n",
        "    return audio_out\n",
        "\n",
        "def ingest_video(path):\n",
        "    audio_path = extract_audio_from_video(path)\n",
        "    return ingest_audio(audio_path)\n",
        "\n",
        "# ROUTER (decides which ingestion to use)\n",
        "def ingest_file(path):\n",
        "    ext = Path(path).suffix.lower()\n",
        "    if ext in [\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\"]:\n",
        "        return ingest_image(path)\n",
        "    elif ext == \".pdf\":\n",
        "        return ingest_pdf(path)\n",
        "    elif ext in [\".mp3\", \".wav\", \".m4a\"]:\n",
        "        return ingest_audio(path)\n",
        "    elif ext in [\".mp4\", \".mov\", \".avi\", \".mkv\"]:\n",
        "        return ingest_video(path)\n",
        "    else:\n",
        "        print(f\"‚ö† Unsupported file: {ext}\")\n",
        "        return None\n",
        "\n",
        "# Batch ingestion\n",
        "def ingest_all(folder=\"uploads\"):\n",
        "    assets = []\n",
        "    for f in os.listdir(folder):\n",
        "        path = os.path.join(folder, f)\n",
        "        print(f\"‚Üí Ingesting {f} ...\")\n",
        "        try:\n",
        "            asset = ingest_file(path)\n",
        "            if asset: assets.append(asset)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error ingesting {f}: {e}\")\n",
        "    print(f\"\\nüì¶ Total ingested: {len(assets)} files\")\n",
        "    return assets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "eVesCfMvLmKF",
        "outputId": "8358c0fb-41d4-4411-d296-67494efb8b1e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c60ffa26-5c4c-40d1-98c1-cbae1b4b7c51\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c60ffa26-5c4c-40d1-98c1-cbae1b4b7c51\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Use this cell to upload files from your local machine into the Colab 'uploads/' folder.\n",
        "# It will open a file picker; you can select multiple files at once.\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "    dest = os.path.join(\"uploads\", fn)\n",
        "    # if already present, it will be overwritten\n",
        "    with open(dest, \"wb\") as out:\n",
        "        out.write(uploaded[fn])\n",
        "    print(f\"‚úî Uploaded {fn} -> uploads/{fn}\")\n",
        "\n",
        "print(\"‚úÖ Upload complete. Now run the batch ingestion cell.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G-ODyPDLo2h"
      },
      "outputs": [],
      "source": [
        "# Batch ingest everything in uploads/\n",
        "def ingest_all(folder=\"uploads\"):\n",
        "    files = sorted(os.listdir(folder))\n",
        "    ingested = []\n",
        "    for f in files:\n",
        "        path = os.path.join(folder, f)\n",
        "        if os.path.isdir(path):\n",
        "            continue\n",
        "        print(f\"‚Üí Ingesting {f} ...\")\n",
        "        a = ingest_file(path)\n",
        "        if a:\n",
        "            print(f\"  ‚úÖ Ingested: {f} (id: {a['id'][:8]}) text length: {len(a['text'] or '')}\")\n",
        "            ingested.append(a)\n",
        "        else:\n",
        "            print(f\"  ‚ö† Skipped/failed: {f}\")\n",
        "    print(f\"\\nüì¶ Total ingested: {len(ingested)}\")\n",
        "    return ingested\n",
        "\n",
        "all_assets = ingest_all(\"uploads\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "advSQXKwLuf1"
      },
      "outputs": [],
      "source": [
        "# Build FAISS index from assets stored in SQLite (or load persisted index)\n",
        "import pickle\n",
        "\n",
        "def build_faiss_index(rebuild=False):\n",
        "    # load all texts\n",
        "    c.execute(\"SELECT id, text_content FROM assets\")\n",
        "    rows = c.fetchall()\n",
        "    ids = []\n",
        "    texts = []\n",
        "    for r in rows:\n",
        "        if r[1] and len(r[1].strip())>0:\n",
        "            ids.append(r[0])\n",
        "            texts.append(r[1])\n",
        "    if len(texts) == 0:\n",
        "        raise RuntimeError(\"No texts found in DB. Ingest files first.\")\n",
        "    # compute embeddings\n",
        "    embs = embed_model.encode(texts, convert_to_numpy=True)\n",
        "    # normalize for cosine similarity with inner product\n",
        "    norms = np.linalg.norm(embs, axis=1, keepdims=True)\n",
        "    embs = embs / (norms + 1e-10)\n",
        "    # build index\n",
        "    index = faiss.IndexFlatIP(embs.shape[1])\n",
        "    index.add(embs)\n",
        "    # save index and id map\n",
        "    faiss.write_index(index, FAISS_INDEX_FILE)\n",
        "    with open(IDMAP_PICKLE, \"wb\") as f:\n",
        "        pickle.dump(ids, f)\n",
        "    print(f\"‚úÖ Built FAISS index with {index.ntotal} vectors.\")\n",
        "    return index, ids, texts\n",
        "\n",
        "# Build (call it)\n",
        "index, id_map, texts = build_faiss_index()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9MHW0cUL3fe"
      },
      "outputs": [],
      "source": [
        "# Build a NetworkX graph linking asset ids to named entities\n",
        "def build_graph():\n",
        "    G.clear()\n",
        "    c.execute(\"SELECT id, filename, text_content FROM assets\")\n",
        "    for aid, fname, txt in c.fetchall():\n",
        "        if not txt:\n",
        "            continue\n",
        "        G.add_node(aid, type=\"asset\", label=fname)\n",
        "        doc = nlp(txt)\n",
        "        for ent in doc.ents:\n",
        "            ent_text = ent.text.strip()\n",
        "            if not ent_text:\n",
        "                continue\n",
        "            if not G.has_node(ent_text):\n",
        "                G.add_node(ent_text, type=\"entity\", label=ent_text, entity_type=ent.label_)\n",
        "            G.add_edge(aid, ent_text, relation=f\"mentions:{ent.label_}\")\n",
        "    print(f\"‚úÖ Graph built: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
        "\n",
        "build_graph()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-GfMFVBL6mb"
      },
      "outputs": [],
      "source": [
        "def search_top_k(question, k=3):\n",
        "    # embed query + normalize\n",
        "    q_emb = embed_model.encode([question], convert_to_numpy=True)\n",
        "    q_emb = q_emb / (np.linalg.norm(q_emb, axis=1, keepdims=True) + 1e-10)\n",
        "    D, I = index.search(q_emb, k)\n",
        "    retrieved = []\n",
        "    for i in I[0]:\n",
        "        if i < 0:\n",
        "            continue\n",
        "        aid = id_map[i]\n",
        "        row = c.execute(\"SELECT filename, text_content FROM assets WHERE id=?\", (aid,)).fetchone()\n",
        "        if row:\n",
        "            retrieved.append({\"id\": aid, \"filename\": row[0], \"text\": row[1]})\n",
        "    return retrieved\n",
        "\n",
        "def find_related_docs(retrieved_assets):\n",
        "    related = set()\n",
        "    for a in retrieved_assets:\n",
        "        doc = nlp(a[\"text\"] or \"\")\n",
        "        for ent in doc.ents:\n",
        "            if ent.text in G:\n",
        "                for neighbor in G.neighbors(ent.text):\n",
        "                    if G.nodes[neighbor].get(\"type\") == \"asset\" and neighbor != a[\"id\"]:\n",
        "                        related.add(G.nodes[neighbor][\"label\"])\n",
        "    return sorted(list(related))\n",
        "\n",
        "def rag_answer(question, k=3):\n",
        "    retrieved = search_top_k(question, k=k)\n",
        "    if not retrieved:\n",
        "        return \"Nessun documento rilevante trovato.\"\n",
        "\n",
        "    # Build context snippet\n",
        "    context_texts = []\n",
        "    supports = []\n",
        "    for r in retrieved:\n",
        "        snippet = (r[\"text\"] or \"\")[:1500]\n",
        "        context_texts.append(f\"[{r['filename']}] {snippet}\")\n",
        "        supports.append(r[\"filename\"])\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Sei un assistente archivistico che risponde in italiano in modo conciso e preciso.\n",
        "Domanda: {question}\n",
        "\n",
        "Contesti recuperati (sintesi):\n",
        "{\"\\n\\n\".join(context_texts)}\n",
        "\n",
        "Istruzioni:\n",
        "1) Fornisci una risposta breve e precisa in italiano.\n",
        "2) Riporta il/i file che supportano la risposta usando i nomi file (ad esempio: Martiny_doc010.jpg).\n",
        "3) Se ci sono altri documenti collegati (es. menzionano le stesse entit√†), elencali.\n",
        "\n",
        "Risposta:\n",
        "\"\"\"\n",
        "\n",
        "    resp = gemini_model.generate_content(prompt)\n",
        "    answer_text = resp.text.strip()\n",
        "    related = find_related_docs(retrieved)\n",
        "    if related:\n",
        "        answer_text += \"\\n\\nCollegato anche a: \" + \", \".join(related)\n",
        "    # Also add the direct supporting assets\n",
        "    answer_text += \"\\n\\nFonte principale: \" + \", \".join(supports)\n",
        "    return answer_text\n",
        "\n",
        "# Example\n",
        "print(\"Esempio:\", rag_answer(\"Qual √® il numero d‚Äôordine citato nella lettera Martiny del 26 maggio 1914?\", k=3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "rZyhB3fGL9l4",
        "outputId": "7fc4c2c3-f187-43cf-91a2-20a565cebab6"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-521335190.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nScrivi la tua domanda (or type 'exit' to stop): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "# Use this cell to type a question interactively.\n",
        "# Run it, then type your query and press Enter.\n",
        "\n",
        "while True:\n",
        "    q = input(\"\\nScrivi la tua domanda (or type 'exit' to stop): \").strip()\n",
        "    if q.lower() in (\"exit\", \"quit\"):\n",
        "        break\n",
        "    print(\"\\n--- Generating answer (may take a few seconds) ---\\n\")\n",
        "    print(rag_answer(q, k=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MV5BUGcXMAPD"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "# Save DB into outputs and download\n",
        "files.download(\"metadata.db\")\n",
        "print(\"Downloaded metadata.db\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}